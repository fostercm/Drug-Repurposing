{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook has 4 Sections:\n",
    "* Data processing\n",
    "* Setting up the model architecture\n",
    "* Defining the training loop\n",
    "* Running a limited test of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Deprecated_Dataset import processData\n",
    "from torch_geometric.nn import to_hetero\n",
    "from torch.optim.lr_scheduler import StepLR, ExponentialLR\n",
    "from torch_geometric import nn\n",
    "from torch_geometric.nn import GATConv\n",
    "import torch\n",
    "from torch.nn import SiLU\n",
    "from DistMultMod import DistMultMod\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use of the general data processing code to produce the graph and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/scratch/labs/wuc/Drug-Repurposing/src/Deprecated_Dataset.py:15: DtypeWarning: Columns (3,8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  kg = pd.read_csv(path + r'/data/kg.csv')\n",
      "/opt/scratch/labs/wuc/Drug-Repurposing/src/Deprecated_Dataset.py:72: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  raw_pretrain_indices['relation'] = raw_pretrain_indices['relation'].map(name_to_num)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HeteroData(\n",
       "  gene_protein={ x=[27671, 256] },\n",
       "  drug={ x=[7957, 256] },\n",
       "  effect_phenotype={ x=[15311, 256] },\n",
       "  disease={ x=[17080, 256] },\n",
       "  biological_process={ x=[28642, 256] },\n",
       "  molecular_function={ x=[11169, 256] },\n",
       "  cellular_component={ x=[4176, 256] },\n",
       "  exposure={ x=[818, 256] },\n",
       "  pathway={ x=[2516, 256] },\n",
       "  anatomy={ x=[14035, 256] },\n",
       "  (anatomy, anatomy_anatomy, anatomy)={ edge_index=[2, 28064] },\n",
       "  (anatomy, anatomy_protein_absent, gene_protein)={ edge_index=[2, 19887] },\n",
       "  (anatomy, anatomy_protein_present, gene_protein)={ edge_index=[2, 1518203] },\n",
       "  (biological_process, bioprocess_bioprocess, biological_process)={ edge_index=[2, 105772] },\n",
       "  (biological_process, bioprocess_protein, gene_protein)={ edge_index=[2, 144805] },\n",
       "  (cellular_component, cellcomp_cellcomp, cellular_component)={ edge_index=[2, 9690] },\n",
       "  (cellular_component, cellcomp_protein, gene_protein)={ edge_index=[2, 83402] },\n",
       "  (disease, contraindication, drug)={ edge_index=[2, 30675] },\n",
       "  (disease, disease_disease, disease)={ edge_index=[2, 64388] },\n",
       "  (disease, disease_phenotype_negative, effect_phenotype)={ edge_index=[2, 1193] },\n",
       "  (disease, disease_phenotype_positive, effect_phenotype)={ edge_index=[2, 150317] },\n",
       "  (disease, disease_protein, gene_protein)={ edge_index=[2, 80411] },\n",
       "  (drug, drug_drug, drug)={ edge_index=[2, 2672628] },\n",
       "  (drug, drug_effect, effect_phenotype)={ edge_index=[2, 64784] },\n",
       "  (drug, drug_protein, gene_protein)={ edge_index=[2, 25653] },\n",
       "  (biological_process, exposure_bioprocess, exposure)={ edge_index=[2, 1625] },\n",
       "  (cellular_component, exposure_cellcomp, exposure)={ edge_index=[2, 10] },\n",
       "  (disease, exposure_disease, exposure)={ edge_index=[2, 2304] },\n",
       "  (exposure, exposure_exposure, exposure)={ edge_index=[2, 4140] },\n",
       "  (exposure, exposure_molfunc, molecular_function)={ edge_index=[2, 45] },\n",
       "  (exposure, exposure_protein, gene_protein)={ edge_index=[2, 1212] },\n",
       "  (disease, indication, drug)={ edge_index=[2, 9388] },\n",
       "  (molecular_function, molfunc_molfunc, molecular_function)={ edge_index=[2, 27148] },\n",
       "  (gene_protein, molfunc_protein, molecular_function)={ edge_index=[2, 69530] },\n",
       "  (disease, off_label_use, drug)={ edge_index=[2, 2568] },\n",
       "  (pathway, pathway_pathway, pathway)={ edge_index=[2, 5070] },\n",
       "  (gene_protein, pathway_protein, pathway)={ edge_index=[2, 42646] },\n",
       "  (effect_phenotype, phenotype_phenotype, effect_phenotype)={ edge_index=[2, 37472] },\n",
       "  (effect_phenotype, phenotype_protein, gene_protein)={ edge_index=[2, 3330] },\n",
       "  (gene_protein, protein_protein, gene_protein)={ edge_index=[2, 642150] },\n",
       "  (gene_protein, rev_anatomy_protein_absent, anatomy)={ edge_index=[2, 19887] },\n",
       "  (gene_protein, rev_anatomy_protein_present, anatomy)={ edge_index=[2, 1518203] },\n",
       "  (gene_protein, rev_bioprocess_protein, biological_process)={ edge_index=[2, 144805] },\n",
       "  (gene_protein, rev_cellcomp_protein, cellular_component)={ edge_index=[2, 83402] },\n",
       "  (drug, rev_contraindication, disease)={ edge_index=[2, 30675] },\n",
       "  (effect_phenotype, rev_disease_phenotype_negative, disease)={ edge_index=[2, 1193] },\n",
       "  (effect_phenotype, rev_disease_phenotype_positive, disease)={ edge_index=[2, 150317] },\n",
       "  (gene_protein, rev_disease_protein, disease)={ edge_index=[2, 80411] },\n",
       "  (effect_phenotype, rev_drug_effect, drug)={ edge_index=[2, 64784] },\n",
       "  (gene_protein, rev_drug_protein, drug)={ edge_index=[2, 25653] },\n",
       "  (exposure, rev_exposure_bioprocess, biological_process)={ edge_index=[2, 1625] },\n",
       "  (exposure, rev_exposure_cellcomp, cellular_component)={ edge_index=[2, 10] },\n",
       "  (exposure, rev_exposure_disease, disease)={ edge_index=[2, 2304] },\n",
       "  (molecular_function, rev_exposure_molfunc, exposure)={ edge_index=[2, 45] },\n",
       "  (gene_protein, rev_exposure_protein, exposure)={ edge_index=[2, 1212] },\n",
       "  (drug, rev_indication, disease)={ edge_index=[2, 9388] },\n",
       "  (molecular_function, rev_molfunc_protein, gene_protein)={ edge_index=[2, 69530] },\n",
       "  (drug, rev_off_label_use, disease)={ edge_index=[2, 2568] },\n",
       "  (pathway, rev_pathway_protein, gene_protein)={ edge_index=[2, 42646] },\n",
       "  (gene_protein, rev_phenotype_protein, effect_phenotype)={ edge_index=[2, 3330] }\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2285 286 286\n",
      "17 3 3\n"
     ]
    }
   ],
   "source": [
    "data, ptrain_loader, pval_loader, ptest_loader, ftrain_loader, fval_loader, ftest_loader, local_idx_map = processData(256,2048)\n",
    "display(data)\n",
    "print(len(ptrain_loader), len(pval_loader), len(ptest_loader))\n",
    "print(len(ftrain_loader), len(fval_loader), len(ftest_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disease similarity search to improve zero-shot embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the one-hot vectors for disease similarity computation\n",
    "def generateOverallOneHot(disease_idx,data):\n",
    "    \n",
    "    # Generate a generic one-hot vector for a condition\n",
    "    def generateOneHot(node_idx, edge_type, data):\n",
    "        \n",
    "        # Get neighbors of the node\n",
    "        edges = np.array(data[edge_type].edge_index)\n",
    "        mask = np.where(edges[0] == node_idx)[0]\n",
    "        neighbors = edges[1][mask]\n",
    "        \n",
    "        # Generate the one-hot vector\n",
    "        one_hot = np.zeros(data[edge_type[2]].num_nodes,dtype=int)\n",
    "        one_hot[neighbors] = 1\n",
    "        return one_hot\n",
    "    \n",
    "    # Generate the one-hot vectors for the disease with important neighbors and concatenate them\n",
    "    geneOneHot = generateOneHot(disease_idx,('disease','disease_protein','gene_protein'),data)\n",
    "    effectOneHot = generateOneHot(disease_idx,('disease', 'disease_phenotype_negative', 'effect_phenotype'),data) + generateOneHot(disease_idx,('disease', 'disease_phenotype_positive', 'effect_phenotype'),data)\n",
    "    exposureOneHot = generateOneHot(disease_idx,('disease', 'exposure_disease', 'exposure'),data)\n",
    "    diseaseOneHot = generateOneHot(disease_idx,('disease','disease_disease','disease'),data)\n",
    "    overallOneHot = torch.tensor(np.hstack([geneOneHot,effectOneHot,exposureOneHot,diseaseOneHot]))\n",
    "    \n",
    "    return overallOneHot\n",
    "\n",
    "def constructDiseaseSimilarity(k=10):\n",
    "    \n",
    "    # Get the number of diseases and the number of possible neighbors\n",
    "    num_diseases = data['disease'].num_nodes\n",
    "    num_possible_neighbors = data['gene_protein'].num_nodes + data['effect_phenotype'].num_nodes + data['exposure'].num_nodes + data['disease'].num_nodes\n",
    "    \n",
    "    # Generate the one-hot vectors for all diseases\n",
    "    oneHots = torch.zeros(num_diseases,num_possible_neighbors)\n",
    "    for disease_idx in tqdm(range(num_diseases)):\n",
    "        oneHots[disease_idx] = generateOverallOneHot(disease_idx,data)\n",
    "    \n",
    "    # Compute the similarity between diseases and store the top-k most similar diseases\n",
    "    disease_similarity_storage = torch.zeros(num_diseases,2,k)\n",
    "    similarity_matrix = torch.zeros(num_diseases,num_diseases)\n",
    "    \n",
    "    for query_disease in tqdm(range(num_diseases)):\n",
    "        \n",
    "        queryOneHot = oneHots[query_disease]\n",
    "        \n",
    "        for key_disease in range(query_disease+1,num_diseases):\n",
    "            \n",
    "            keyOneHot = oneHots[key_disease]\n",
    "            similarity = torch.dot(queryOneHot,keyOneHot)\n",
    "            similarity_matrix[query_disease][key_disease] = similarity\n",
    "            similarity_matrix[key_disease][query_disease] = similarity\n",
    "    \n",
    "    for query_disease in tqdm(range(num_diseases)):\n",
    "        \n",
    "        similarity = similarity_matrix[query_disease]\n",
    "        \n",
    "        # Get the top-k most similar diseases to the query disease and store them    \n",
    "        topk = torch.topk(similarity_matrix[query_disease],k)\n",
    "        if torch.sum(topk.values) == 0:\n",
    "            disease_similarity_storage[query_disease][0] = torch.zeros(k)\n",
    "        else:\n",
    "            disease_similarity_storage[query_disease][0] = topk.values / torch.sum(topk.values)\n",
    "        disease_similarity_storage[query_disease][1] = topk.indices\n",
    "        \n",
    "    return disease_similarity_storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11083 11269 11107"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'disease_similarity.pt' not in os.listdir():\n",
    "    torch.save(constructDiseaseSimilarity(),'disease_similarity.pt') # Takes about 40 mins to run, only need to run once"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model will have a GAT-based encoder to produce node embeddings, with a DistMult-based decoder for the purpose of link prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KGLinkPredictor(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, data, similarity_path, local_idx_map):\n",
    "        super(KGLinkPredictor, self).__init__()\n",
    "        \n",
    "        self.Encoder = nn.Sequential('x, edge_index', [\n",
    "            (GATConv(in_channels,hidden_channels,add_self_loops=False), 'x, edge_index -> x'),\n",
    "            SiLU(inplace=True),\n",
    "            (GATConv(hidden_channels,hidden_channels,add_self_loops=False), 'x, edge_index -> x'),\n",
    "            SiLU(inplace=True),\n",
    "            (GATConv(hidden_channels,hidden_channels,add_self_loops=False), 'x, edge_index -> x'),\n",
    "            SiLU(inplace=True),\n",
    "            (GATConv(hidden_channels,hidden_channels,add_self_loops=False), 'x, edge_index -> x'),\n",
    "            SiLU(inplace=True),\n",
    "            (GATConv(hidden_channels,hidden_channels,add_self_loops=False), 'x, edge_index -> x'),\n",
    "            SiLU(inplace=True),\n",
    "            (GATConv(hidden_channels,hidden_channels,add_self_loops=False), 'x, edge_index -> x')\n",
    "        ])\n",
    "        \n",
    "        self.Decoder = DistMultMod(data.num_nodes, data.num_edges, hidden_channels, data, similarity_path, local_idx_map)\n",
    "        \n",
    "        self.data = data\n",
    "\n",
    "    def forward(self, head_indices, relations, tail_indices):\n",
    "        x = self.Encoder(self.data.x_dict,self.data.edge_index_dict)\n",
    "        self.Decoder.node_emb = torch.vstack([*x.values()])\n",
    "        return torch.sigmoid(self.Decoder(head_indices, relations, tail_indices))\n",
    "    \n",
    "    def loss(self, head_index, relation, tail_index):\n",
    "        return self.Decoder.loss(head_index, relation, tail_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we do a full-relation pre-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data, train_dataLoader, val_dataloader, model, optimizer, scheduler, device):\n",
    "    train_loss = 0\n",
    "    for i,idx_data in enumerate(train_dataLoader):\n",
    "        idx_data = idx_data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        model(idx_data[:,0], idx_data[:,1], idx_data[:,2])\n",
    "        loss = model.loss(idx_data[:,0], idx_data[:,1], idx_data[:,2])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        # if i % 10 == 0:\n",
    "        #     print(f\"Train Batch {i+1} loss | {loss.item()}\")\n",
    "        # print(f\"Train Batch {i+1} loss | {loss.item()}\")\n",
    "    # scheduler.step()\n",
    "    \n",
    "    val_loss = 0\n",
    "    for i,idx_data in enumerate(val_dataloader):\n",
    "        idx_data = idx_data.to(device)\n",
    "        loss = model.loss(idx_data[:,0], idx_data[:,1], idx_data[:,2])\n",
    "        val_loss += loss.item()\n",
    "        # print(f\"Val Batch {i+1} loss | {loss.item()}\")\n",
    "        \n",
    "    return train_loss / len(train_dataLoader), val_loss / len(val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will run the model once on our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1771, 0.8381, 0.2668], device='cuda:0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor(1.0918, device='cuda:0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data.to('cuda')\n",
    "testModel = KGLinkPredictor(256,64,data,'disease_similarity.pt', local_idx_map).to('cuda')\n",
    "testModel.Encoder = to_hetero(testModel.Encoder,data.metadata())\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = testModel(torch.tensor([0,0,2]), torch.tensor([0,1,2]), torch.tensor([2,1,3]))\n",
    "    loss = testModel.loss(torch.tensor([0,0,2]), torch.tensor([0,1,2]), torch.tensor([2,1,3]))\n",
    "\n",
    "display(out,loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Test Run for a couple batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.12267233642629252, 0.0942043760022917)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "data.to(device)\n",
    "testModel.to(device)\n",
    "testOptimizer = torch.optim.Adam(testModel.parameters(), lr=0.001)\n",
    "testScheduler = ExponentialLR(testOptimizer, gamma=0.1)\n",
    "\n",
    "train(data, ptrain_loader, pval_loader, testModel, testOptimizer, testScheduler, device) # I estimate around 25 mins/epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "testModel.Decoder.reset_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c8bf3a5126d4421a40e5a50c130e3a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Loss: 0.04879272641504512 | Val Loss: 0.050193200508753456\n",
      "Epoch 2 | Train Loss: 0.04923012743101401 | Val Loss: 0.04971953108906746\n",
      "Epoch 3 | Train Loss: 0.051049404284533334 | Val Loss: 0.056397306422392525\n",
      "Epoch 4 | Train Loss: 0.04690680565202937 | Val Loss: 0.0409269571925203\n",
      "Epoch 5 | Train Loss: 0.04652618529165492 | Val Loss: 0.05027736102541288\n",
      "Epoch 6 | Train Loss: 0.0464226261657827 | Val Loss: 0.04729254295428594\n",
      "Epoch 7 | Train Loss: 0.043016956133000994 | Val Loss: 0.06325493132074674\n",
      "Epoch 8 | Train Loss: 0.04566275328397751 | Val Loss: 0.05355566864212354\n",
      "Epoch 9 | Train Loss: 0.04513511828639928 | Val Loss: 0.04592655785381794\n",
      "Epoch 10 | Train Loss: 0.051285226117162144 | Val Loss: 0.06168110171953837\n",
      "Epoch 11 | Train Loss: 0.04599643827361219 | Val Loss: 0.048447027802467346\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m20\u001b[39m)):\n\u001b[0;32m----> 2\u001b[0m     train_loss, val_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mftrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtestModel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtestOptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtestScheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# I estimate around 2 mins/epoch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Val Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[7], line 8\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(data, train_dataLoader, val_dataloader, model, optimizer, scheduler, device)\u001b[0m\n\u001b[1;32m      6\u001b[0m model(idx_data[:,\u001b[38;5;241m0\u001b[39m], idx_data[:,\u001b[38;5;241m1\u001b[39m], idx_data[:,\u001b[38;5;241m2\u001b[39m])\n\u001b[1;32m      7\u001b[0m loss \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mloss(idx_data[:,\u001b[38;5;241m0\u001b[39m], idx_data[:,\u001b[38;5;241m1\u001b[39m], idx_data[:,\u001b[38;5;241m2\u001b[39m])\n\u001b[0;32m----> 8\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     10\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m/home/labs/wuc/Drug-Repurposing/.venv/lib/python3.8/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/labs/wuc/Drug-Repurposing/.venv/lib/python3.8/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/labs/wuc/Drug-Repurposing/.venv/lib/python3.8/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(20)):\n",
    "    train_loss, val_loss = train(data, ftrain_loader, fval_loader, testModel, testOptimizer, testScheduler, device) # I estimate around 2 mins/epoch\n",
    "    print(f\"Epoch {epoch+1} | Train Loss: {train_loss} | Val Loss: {val_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the embeddings being modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Meaningful embedding '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([-34.0878, -10.9848,   3.2778,  15.8652,   1.1965, -57.8402,  14.5811,\n",
       "         -3.2915,  12.9621,  11.9062,  -6.9801,   9.7309,   6.8755,   6.1691,\n",
       "         -8.6140, -11.2621,  -5.6652,   1.9878,  -5.0708, -11.2943,   8.9231,\n",
       "          8.1831,  12.9974, -33.6114,  -3.9108,   2.9071, -14.2387,  52.5912,\n",
       "         -2.3146,  13.5081,  -6.8878,  52.1212,  26.7223, -17.1594, -23.2313,\n",
       "         27.5976,  19.7402,  -4.6048, -11.9684,  -8.2866,  -8.7745,   1.0524,\n",
       "        -20.5567,  17.9088,  19.5952,  16.3532,  -6.2202, -16.1388,  -0.0686,\n",
       "         25.3577,  -7.3419,  -0.4495,  33.4917,   8.2671,   3.1699,  11.9944,\n",
       "         -3.9773,  -2.8941, -16.1103,   0.1276,  -4.9487, -17.1192,   2.1534,\n",
       "         -5.7091], device='cuda:0', grad_fn=<SelectBackward0>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Randomly initialized tensor '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([-7.9961e-03, -6.1600e-03, -3.3313e-03, -2.1216e-03,  2.9461e-03,\n",
       "        -1.4158e-03,  4.1348e-03,  1.4422e-02, -2.8717e-03,  1.2612e-03,\n",
       "        -6.2864e-03, -5.6060e-03, -1.4166e-02, -8.7319e-03,  7.5883e-03,\n",
       "        -1.6285e-03, -4.0831e-03,  7.6047e-03,  4.7692e-03, -3.3069e-03,\n",
       "         3.9118e-03,  1.0287e-03,  1.1578e-02,  1.4036e-02,  1.3397e-02,\n",
       "         1.4138e-02, -1.0554e-05, -7.0151e-03, -4.7827e-04,  7.1407e-04,\n",
       "         5.6455e-03, -6.3677e-03, -1.1812e-02, -2.7951e-03, -3.0000e-03,\n",
       "        -1.3138e-02, -5.3270e-04, -1.0024e-02, -7.1597e-03, -1.0843e-02,\n",
       "         3.4279e-03,  1.4008e-02, -5.3258e-03,  1.3800e-02,  9.4782e-03,\n",
       "         1.3524e-02, -3.5637e-03,  5.8734e-03,  3.4998e-03,  5.4336e-03,\n",
       "        -1.1303e-02, -4.7218e-03, -1.0765e-04,  1.2134e-02, -5.3067e-03,\n",
       "         7.1311e-04,  1.0168e-02, -3.8810e-03,  6.9090e-03,  7.4683e-03,\n",
       "         5.2419e-03,  9.5700e-03, -9.6588e-03, -1.0856e-02,  1.4338e-02,\n",
       "         1.1300e-02, -9.1476e-03, -9.2345e-03, -1.7638e-03, -1.0874e-02,\n",
       "         4.2499e-03,  4.0239e-03, -1.0429e-02,  3.6337e-03, -2.9856e-03,\n",
       "        -6.0852e-03, -5.4741e-03,  1.4464e-02, -1.9438e-03,  1.2544e-02,\n",
       "        -7.7378e-03, -5.3345e-03, -1.9000e-03,  7.2013e-03, -1.1404e-02,\n",
       "         1.4340e-02,  6.5828e-03,  1.4013e-02, -1.1907e-02, -1.0793e-02,\n",
       "        -3.1432e-04,  2.8111e-03,  8.0367e-03, -9.4300e-03, -1.2517e-03,\n",
       "         9.8217e-03, -8.0466e-03,  1.2570e-02, -6.4734e-03, -7.9987e-03,\n",
       "        -6.0303e-03,  5.9579e-03, -1.1955e-02,  1.1472e-02,  4.5619e-03,\n",
       "        -2.2164e-03,  9.3862e-03,  7.8495e-03, -5.3286e-03, -9.2360e-03,\n",
       "         2.1713e-03, -2.8003e-04,  1.0998e-02, -6.3500e-03, -9.5459e-03,\n",
       "        -7.6260e-03, -6.0383e-03, -1.3213e-02,  8.0072e-03,  1.3304e-02,\n",
       "         1.0876e-02,  6.0608e-03, -2.5838e-03,  1.3222e-02,  7.4518e-03,\n",
       "         1.3511e-02,  1.9587e-03, -3.9756e-03,  5.0114e-03,  1.4375e-02,\n",
       "        -1.3522e-02, -1.3547e-02,  1.1393e-02, -1.2319e-02,  1.0383e-02,\n",
       "        -8.9331e-03,  1.0148e-02,  1.4076e-03, -7.8059e-03,  5.5627e-03,\n",
       "        -9.9481e-03, -1.2176e-02, -1.0354e-02,  6.5575e-03, -2.4842e-03,\n",
       "         1.3934e-02,  1.0061e-02,  1.1900e-02, -7.4722e-04, -4.2145e-03,\n",
       "         3.9116e-03, -4.1428e-03,  6.4507e-03,  1.2919e-02,  1.2753e-02,\n",
       "        -5.7667e-03, -5.1817e-03, -4.1221e-03, -1.3317e-03,  9.4015e-03,\n",
       "         1.4426e-02,  1.4320e-03,  1.1683e-02,  1.1000e-02,  1.0300e-02,\n",
       "        -3.0021e-03, -1.5439e-03,  1.9597e-04, -1.2797e-02,  3.7894e-03,\n",
       "        -3.0357e-03,  9.3740e-03,  3.0543e-03, -8.7949e-03, -1.9156e-03,\n",
       "         2.8821e-03,  8.8621e-03,  6.5466e-03, -9.5027e-03, -6.0403e-03,\n",
       "        -1.1553e-02,  6.1202e-03, -7.6223e-03,  1.5528e-03, -1.0384e-02,\n",
       "         2.5928e-03,  1.2163e-02, -6.2202e-04,  9.6990e-03, -3.4638e-03,\n",
       "        -5.7238e-03,  1.3167e-02,  9.9087e-03, -5.3676e-04,  5.4732e-03,\n",
       "         2.8800e-03, -1.0774e-02, -1.6507e-03,  4.7002e-03, -6.3492e-03,\n",
       "        -5.2907e-03,  4.0228e-03, -6.5548e-03,  9.4474e-03,  1.3055e-03,\n",
       "        -4.4234e-03,  2.1921e-04,  5.9539e-03, -4.4730e-03, -3.1972e-03,\n",
       "        -2.4773e-03,  8.1156e-03, -1.2920e-02,  6.1813e-03,  1.3010e-02,\n",
       "        -1.4387e-02,  1.3878e-02, -1.2069e-02, -3.7237e-03, -4.2399e-04,\n",
       "         1.0483e-03,  3.0011e-04,  1.0401e-02, -6.6969e-03, -8.1871e-03,\n",
       "         1.2528e-02, -7.5233e-03,  6.2522e-03,  2.1748e-03, -1.4485e-02,\n",
       "        -1.4234e-03, -4.7826e-03,  1.4190e-02,  1.2303e-03, -7.1670e-03,\n",
       "        -4.4570e-03, -1.3447e-02,  1.1016e-02,  1.0499e-02, -1.0802e-03,\n",
       "         7.9796e-03, -1.3387e-04, -2.7385e-04,  1.1223e-02, -1.0887e-02,\n",
       "         2.7424e-03,  2.2864e-03,  6.5741e-03, -1.1068e-02,  1.1313e-03,\n",
       "         3.0991e-04,  1.1262e-02,  1.0606e-02, -4.6533e-03, -1.2868e-02,\n",
       "         7.5934e-03], device='cuda:0', grad_fn=<SelectBackward0>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(\"Meaningful embedding \", testModel.Encoder(testModel.data.x_dict,testModel.data.edge_index_dict)['gene_protein'][0])\n",
    "display(\"Randomly initialized tensor \", testModel.data.x_dict['gene_protein'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Edge predictions change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1713, 0.8381, 0.3036], device='cuda:0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    display(torch.sigmoid(testModel.Decoder(torch.tensor([0,0,2]), torch.tensor([0,1,2]), torch.tensor([2,1,3]))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
