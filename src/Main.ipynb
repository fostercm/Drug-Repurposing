{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Dataset import processData\n",
    "from Model import train, KGLinkPredictor\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from path: /opt/scratch/labs/wuc/Drug-Repurposing/data/kg.csv\n",
      "Processing node data...\n",
      "Processing edge data...\n",
      "Setting up pretraining data...\n",
      "Setting up finetuning data...\n",
      "Data processing complete.\n"
     ]
    }
   ],
   "source": [
    "data, ptrain_loader, pval_loader, ftrain_loader, fval_loader, ftest_loader = processData(256,2048) # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declare and Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretraining...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b29322acbe4e4f069fbac190485d2466",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Average Training Loss: 0.14158970243103916 Average Validation Loss: 0.09911965072102896\n",
      "Fine-tuning...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88d89cdb267e491aaf9a4d26f025419b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Average Training Loss: 0.8771406394593856 Average Validation Loss: 0.2618066022793452\n",
      "Epoch 2 | Average Training Loss: 0.20643489764017217 Average Validation Loss: 0.2327298472325007\n",
      "Epoch 3 | Average Training Loss: 0.1772311005522223 Average Validation Loss: 0.21591920653978983\n",
      "Epoch 4 | Average Training Loss: 0.1720651125206667 Average Validation Loss: 0.1823930392662684\n",
      "Epoch 5 | Average Training Loss: 0.1596242724096074 Average Validation Loss: 0.17535688479741415\n",
      "Epoch 6 | Average Training Loss: 0.15083062561119304 Average Validation Loss: 0.18323520322640738\n",
      "Epoch 7 | Average Training Loss: 0.1399695974062471 Average Validation Loss: 0.14557861785093942\n",
      "Epoch 8 | Average Training Loss: 0.1377599874840063 Average Validation Loss: 0.1500058670838674\n",
      "Epoch 9 | Average Training Loss: 0.12417588821228813 Average Validation Loss: 0.15469802419344583\n",
      "Epoch 10 | Average Training Loss: 0.12841125752995997 Average Validation Loss: 0.16495227813720703\n",
      "Epoch 11 | Average Training Loss: 0.12114888079026166 Average Validation Loss: 0.16416010757287344\n",
      "Epoch 12 | Average Training Loss: 0.12395617935587377 Average Validation Loss: 0.15510466446479163\n",
      "Epoch 13 | Average Training Loss: 0.11707885957816068 Average Validation Loss: 0.13341473042964935\n",
      "Epoch 14 | Average Training Loss: 0.11029891362961601 Average Validation Loss: 0.12722630302111307\n",
      "Epoch 15 | Average Training Loss: 0.10779131203889847 Average Validation Loss: 0.12576389064391455\n",
      "Epoch 16 | Average Training Loss: 0.10488418884137098 Average Validation Loss: 0.1162404716014862\n",
      "Epoch 17 | Average Training Loss: 0.10595743095173556 Average Validation Loss: 0.1296385650833448\n",
      "Epoch 18 | Average Training Loss: 0.10137550971087288 Average Validation Loss: 0.09565456211566925\n",
      "Epoch 19 | Average Training Loss: 0.10147596851867788 Average Validation Loss: 0.11031227807203929\n",
      "Epoch 20 | Average Training Loss: 0.0922287899781676 Average Validation Loss: 0.11570841819047928\n",
      "Epoch 21 | Average Training Loss: 0.09510880708694458 Average Validation Loss: 0.11966004470984141\n",
      "Epoch 22 | Average Training Loss: 0.09039097279310226 Average Validation Loss: 0.09435900052388509\n",
      "Epoch 23 | Average Training Loss: 0.09166343203362297 Average Validation Loss: 0.10438579320907593\n",
      "Epoch 24 | Average Training Loss: 0.0875455412794562 Average Validation Loss: 0.11974074443181355\n",
      "Epoch 25 | Average Training Loss: 0.09125029646298465 Average Validation Loss: 0.09280125548442204\n",
      "Epoch 26 | Average Training Loss: 0.08829516785986283 Average Validation Loss: 0.09611845513184865\n",
      "Epoch 27 | Average Training Loss: 0.0845444018349928 Average Validation Loss: 0.08913388848304749\n",
      "Epoch 28 | Average Training Loss: 0.08181895315647125 Average Validation Loss: 0.09372598926226298\n",
      "Epoch 29 | Average Training Loss: 0.08051259070634842 Average Validation Loss: 0.10033432145913442\n",
      "Epoch 30 | Average Training Loss: 0.07896734072881587 Average Validation Loss: 0.0757078304886818\n",
      "Epoch 31 | Average Training Loss: 0.0780110495055423 Average Validation Loss: 0.09713730464378993\n",
      "Epoch 32 | Average Training Loss: 0.07685978140901117 Average Validation Loss: 0.0765841156244278\n",
      "Epoch 33 | Average Training Loss: 0.0770384099553613 Average Validation Loss: 0.09920575718084972\n",
      "Epoch 34 | Average Training Loss: 0.07595117433982737 Average Validation Loss: 0.06931909422079723\n",
      "Epoch 35 | Average Training Loss: 0.07511538332876037 Average Validation Loss: 0.0849766160051028\n",
      "Epoch 36 | Average Training Loss: 0.07463455682291704 Average Validation Loss: 0.076832781235377\n",
      "Epoch 37 | Average Training Loss: 0.06909884621991831 Average Validation Loss: 0.09639347344636917\n",
      "Epoch 38 | Average Training Loss: 0.07111791083041359 Average Validation Loss: 0.09155544638633728\n",
      "Epoch 39 | Average Training Loss: 0.07021316228544011 Average Validation Loss: 0.09746541827917099\n",
      "Epoch 40 | Average Training Loss: 0.07410388483720667 Average Validation Loss: 0.10132375359535217\n",
      "Epoch 41 | Average Training Loss: 0.06992715838200905 Average Validation Loss: 0.07655741274356842\n",
      "Epoch 42 | Average Training Loss: 0.06879157441503861 Average Validation Loss: 0.0867789884408315\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m model\u001b[38;5;241m.\u001b[39mDecoder\u001b[38;5;241m.\u001b[39mreset_parameters()\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFine-tuning...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mftrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/scratch/labs/wuc/Drug-Repurposing/src/Model.py:197\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(train_loader, val_loader, model, optimizer, device, epochs)\u001b[0m\n\u001b[1;32m    194\u001b[0m head_indices,relations,tail_indices \u001b[38;5;241m=\u001b[39m head_indices\u001b[38;5;241m.\u001b[39mto(device),relations\u001b[38;5;241m.\u001b[39mto(device),tail_indices\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# Forward pass through model to update node embeddings\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43mrelations\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtail_indices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[1;32m    200\u001b[0m loss \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mloss(head_indices, relations, tail_indices)\n",
      "File \u001b[0;32m/home/labs/wuc/Drug-Repurposing/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/labs/wuc/Drug-Repurposing/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/scratch/labs/wuc/Drug-Repurposing/src/Model.py:39\u001b[0m, in \u001b[0;36mKGLinkPredictor.forward\u001b[0;34m(self, head_indices, relations, tail_indices)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mDecoder\u001b[38;5;241m.\u001b[39mnode_emb \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mvstack([\u001b[38;5;241m*\u001b[39mx\u001b[38;5;241m.\u001b[39mvalues()])\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Return prediction\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39msigmoid(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrelations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtail_indices\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/home/labs/wuc/Drug-Repurposing/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/labs/wuc/Drug-Repurposing/.venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/scratch/labs/wuc/Drug-Repurposing/src/Model.py:86\u001b[0m, in \u001b[0;36mDistMultMod.forward\u001b[0;34m(self, head_indices, rel_types, tail_indices, loss_tag)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i,relation \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(rel_types):\n\u001b[1;32m     81\u001b[0m     \n\u001b[1;32m     82\u001b[0m     \u001b[38;5;66;03m# Check if relation is a drug-disease relation\u001b[39;00m\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m loss_tag \u001b[38;5;129;01mand\u001b[39;00m (relation \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m relation \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m relation \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m):\n\u001b[1;32m     84\u001b[0m         \n\u001b[1;32m     85\u001b[0m         \u001b[38;5;66;03m# Get head index and modify disease embedding\u001b[39;00m\n\u001b[0;32m---> 86\u001b[0m         head_index \u001b[38;5;241m=\u001b[39m \u001b[43mhead_indices\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_emb[head_index] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodifyDiseaseEmbedding(head_index, relation)\n\u001b[1;32m     89\u001b[0m head \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_emb[head_indices]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = KGLinkPredictor(256,64,data).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "data = data.to(device)\n",
    "\n",
    "print('Pretraining...')\n",
    "train(ptrain_loader,pval_loader, model, optimizer, device, 1) # Takes about 30 mins\n",
    "\n",
    "model.Decoder.reset_parameters()\n",
    "\n",
    "print('Fine-tuning...')\n",
    "train(ftrain_loader,fval_loader, model, optimizer, device, 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
