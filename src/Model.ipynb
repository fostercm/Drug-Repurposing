{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook has 3 purposes:\n",
    "* Setting up the model architecture\n",
    "* Defining the training loop\n",
    "* Running a limited test of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric import nn\n",
    "from torch_geometric.nn import GATConv\n",
    "import torch\n",
    "from torch.nn import SiLU\n",
    "from DistMultMod import DistMultMod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model will have a GAT-based encoder to produce node embeddings, with a DistMult-based decoder for the purpose of link prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KGLinkPredictor(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, data):\n",
    "        super(KGLinkPredictor, self).__init__()\n",
    "        \n",
    "        self.Encoder = nn.Sequential('x, edge_index', [\n",
    "            (GATConv(in_channels,hidden_channels,add_self_loops=False), 'x, edge_index -> x'),\n",
    "            SiLU(inplace=True),\n",
    "            (GATConv(hidden_channels,hidden_channels,add_self_loops=False), 'x, edge_index -> x'),\n",
    "            SiLU(inplace=True),\n",
    "            (GATConv(hidden_channels,hidden_channels,add_self_loops=False), 'x, edge_index -> x')\n",
    "        ])\n",
    "        \n",
    "        self.Decoder = DistMultMod(data.num_nodes, data.num_edges, hidden_channels, data)\n",
    "        \n",
    "        self.data = data\n",
    "\n",
    "    def forward(self, head_indices, relations, tail_indices):\n",
    "        x = self.Encoder(self.data.x_dict,self.data.edge_index_dict)\n",
    "        self.Decoder.node_emb = torch.vstack([*x.values()])\n",
    "        return torch.sigmoid(self.Decoder(head_indices, relations, tail_indices))\n",
    "    \n",
    "    def loss(self, head_index, relation, tail_index):\n",
    "        return self.Decoder.loss(head_index, relation, tail_index)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we do a full-relation pre-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data, train_dataLoader, val_dataloader, model, optimizer, scheduler, device):\n",
    "    train_loss = 0\n",
    "    for i,idx_data in enumerate(train_dataLoader):\n",
    "        idx_data = idx_data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        model(idx_data[:,0], idx_data[:,1], idx_data[:,2])\n",
    "        loss = model.loss(idx_data[:,0], idx_data[:,1], idx_data[:,2])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # scheduler.step()\n",
    "        train_loss += loss.item() / len(idx_data)\n",
    "        if i % 10 == 0:\n",
    "            print(\"Batch loss \", loss.item())\n",
    "    \n",
    "    val_loss = 0\n",
    "    for idx_data in val_dataloader:\n",
    "        idx_data = idx_data.to(device)\n",
    "        loss = model.loss(idx_data[:,0], idx_data[:,1], idx_data[:,2])\n",
    "        val_loss += loss.item() / len(idx_data)\n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Dataset import processData\n",
    "from torch_geometric.nn import to_hetero\n",
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import and process our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\colef\\OneDrive - University of Miami\\Documents\\GitHub\\Drug-Repurposing\\src\\Dataset.py:15: DtypeWarning: Columns (3,8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  kg = pd.read_csv(path + r'\\data\\kg.csv')\n",
      "c:\\Users\\colef\\OneDrive - University of Miami\\Documents\\GitHub\\Drug-Repurposing\\src\\Dataset.py:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  pretrain_indices['relation'] = pretrain_indices['relation'].map(name_to_num)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HeteroData(\n",
       "  gene_protein={ x=[27671, 128] },\n",
       "  drug={ x=[7957, 128] },\n",
       "  effect_phenotype={ x=[15311, 128] },\n",
       "  disease={ x=[17080, 128] },\n",
       "  biological_process={ x=[28642, 128] },\n",
       "  molecular_function={ x=[11169, 128] },\n",
       "  cellular_component={ x=[4176, 128] },\n",
       "  exposure={ x=[818, 128] },\n",
       "  pathway={ x=[2516, 128] },\n",
       "  anatomy={ x=[14035, 128] },\n",
       "  (anatomy, anatomy_anatomy, anatomy)={ edge_index=[2, 28064] },\n",
       "  (gene_protein, anatomy_protein_absent, anatomy)={ edge_index=[2, 39774] },\n",
       "  (gene_protein, anatomy_protein_present, anatomy)={ edge_index=[2, 3036406] },\n",
       "  (biological_process, bioprocess_bioprocess, biological_process)={ edge_index=[2, 105772] },\n",
       "  (gene_protein, bioprocess_protein, biological_process)={ edge_index=[2, 289610] },\n",
       "  (cellular_component, cellcomp_cellcomp, cellular_component)={ edge_index=[2, 9690] },\n",
       "  (gene_protein, cellcomp_protein, cellular_component)={ edge_index=[2, 166804] },\n",
       "  (drug, contraindication, disease)={ edge_index=[2, 61350] },\n",
       "  (disease, disease_disease, disease)={ edge_index=[2, 64388] },\n",
       "  (disease, disease_phenotype_negative, effect_phenotype)={ edge_index=[2, 2386] },\n",
       "  (disease, disease_phenotype_positive, effect_phenotype)={ edge_index=[2, 300634] },\n",
       "  (gene_protein, disease_protein, disease)={ edge_index=[2, 160822] },\n",
       "  (drug, drug_drug, drug)={ edge_index=[2, 2672628] },\n",
       "  (drug, drug_effect, effect_phenotype)={ edge_index=[2, 129568] },\n",
       "  (drug, drug_protein, gene_protein)={ edge_index=[2, 51306] },\n",
       "  (exposure, exposure_bioprocess, biological_process)={ edge_index=[2, 3250] },\n",
       "  (exposure, exposure_cellcomp, cellular_component)={ edge_index=[2, 20] },\n",
       "  (exposure, exposure_disease, disease)={ edge_index=[2, 4608] },\n",
       "  (exposure, exposure_exposure, exposure)={ edge_index=[2, 4140] },\n",
       "  (exposure, exposure_molfunc, molecular_function)={ edge_index=[2, 90] },\n",
       "  (exposure, exposure_protein, gene_protein)={ edge_index=[2, 2424] },\n",
       "  (drug, indication, disease)={ edge_index=[2, 18776] },\n",
       "  (molecular_function, molfunc_molfunc, molecular_function)={ edge_index=[2, 27148] },\n",
       "  (gene_protein, molfunc_protein, molecular_function)={ edge_index=[2, 139060] },\n",
       "  (drug, off_label_use, disease)={ edge_index=[2, 5136] },\n",
       "  (pathway, pathway_pathway, pathway)={ edge_index=[2, 5070] },\n",
       "  (gene_protein, pathway_protein, pathway)={ edge_index=[2, 85292] },\n",
       "  (effect_phenotype, phenotype_phenotype, effect_phenotype)={ edge_index=[2, 37472] },\n",
       "  (gene_protein, phenotype_protein, effect_phenotype)={ edge_index=[2, 6660] },\n",
       "  (gene_protein, protein_protein, gene_protein)={ edge_index=[2, 642150] },\n",
       "  (anatomy, rev_anatomy_protein_absent, gene_protein)={ edge_index=[2, 39774] },\n",
       "  (anatomy, rev_anatomy_protein_present, gene_protein)={ edge_index=[2, 3036406] },\n",
       "  (biological_process, rev_bioprocess_protein, gene_protein)={ edge_index=[2, 289610] },\n",
       "  (cellular_component, rev_cellcomp_protein, gene_protein)={ edge_index=[2, 166804] },\n",
       "  (disease, rev_contraindication, drug)={ edge_index=[2, 61350] },\n",
       "  (effect_phenotype, rev_disease_phenotype_negative, disease)={ edge_index=[2, 2386] },\n",
       "  (effect_phenotype, rev_disease_phenotype_positive, disease)={ edge_index=[2, 300634] },\n",
       "  (disease, rev_disease_protein, gene_protein)={ edge_index=[2, 160822] },\n",
       "  (effect_phenotype, rev_drug_effect, drug)={ edge_index=[2, 129568] },\n",
       "  (gene_protein, rev_drug_protein, drug)={ edge_index=[2, 51306] },\n",
       "  (biological_process, rev_exposure_bioprocess, exposure)={ edge_index=[2, 3250] },\n",
       "  (cellular_component, rev_exposure_cellcomp, exposure)={ edge_index=[2, 20] },\n",
       "  (disease, rev_exposure_disease, exposure)={ edge_index=[2, 4608] },\n",
       "  (molecular_function, rev_exposure_molfunc, exposure)={ edge_index=[2, 90] },\n",
       "  (gene_protein, rev_exposure_protein, exposure)={ edge_index=[2, 2424] },\n",
       "  (disease, rev_indication, drug)={ edge_index=[2, 18776] },\n",
       "  (molecular_function, rev_molfunc_protein, gene_protein)={ edge_index=[2, 139060] },\n",
       "  (disease, rev_off_label_use, drug)={ edge_index=[2, 5136] },\n",
       "  (pathway, rev_pathway_protein, gene_protein)={ edge_index=[2, 85292] },\n",
       "  (effect_phenotype, rev_phenotype_protein, gene_protein)={ edge_index=[2, 6660] }\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50629 6329 6329\n",
      "533 67 67\n"
     ]
    }
   ],
   "source": [
    "data, ptrain_loader, pval_loader, ptest_loader, ftrain_loader, fval_loader, ftest_loader = processData(128,128)\n",
    "display(data)\n",
    "print(len(ptrain_loader), len(pval_loader), len(ptest_loader))\n",
    "print(len(ftrain_loader), len(fval_loader), len(ftest_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will run the model once on our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5000, 0.5000, 0.5000], device='cuda:0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor(1., device='cuda:0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "testModel = KGLinkPredictor(128,8,data).to(device)\n",
    "data.to(device)\n",
    "testModel.Encoder = to_hetero(testModel.Encoder,data.metadata())\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = testModel(torch.tensor([0,0,2]), torch.tensor([0,1,2]), torch.tensor([2,1,3]))\n",
    "    loss = testModel.loss(torch.tensor([0,0,2]), torch.tensor([0,1,2]), torch.tensor([2,1,3]))\n",
    "\n",
    "display(out,loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Test Run for a couple batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss  0.35647904872894287\n",
      "Batch loss  0.3337223529815674\n",
      "Batch loss  0.35349926352500916\n",
      "Batch loss  0.3597087264060974\n",
      "Batch loss  0.3532677888870239\n",
      "Batch loss  0.28600096702575684\n",
      "Batch loss  0.27098560333251953\n",
      "Batch loss  0.371176153421402\n",
      "Batch loss  0.30356529355049133\n",
      "Batch loss  0.3972756862640381\n",
      "Batch loss  0.3064011335372925\n",
      "Batch loss  0.4131999611854553\n",
      "Batch loss  0.32202398777008057\n",
      "Batch loss  0.2591433525085449\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m testOptimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(testModel\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m      4\u001b[0m testScheduler \u001b[38;5;241m=\u001b[39m StepLR(testOptimizer, step_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mptrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtestModel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtestOptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtestScheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[11], line 8\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(data, train_dataLoader, val_dataloader, model, optimizer, scheduler, device)\u001b[0m\n\u001b[0;32m      6\u001b[0m model(idx_data[:,\u001b[38;5;241m0\u001b[39m], idx_data[:,\u001b[38;5;241m1\u001b[39m], idx_data[:,\u001b[38;5;241m2\u001b[39m])\n\u001b[0;32m      7\u001b[0m loss \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mloss(idx_data[:,\u001b[38;5;241m0\u001b[39m], idx_data[:,\u001b[38;5;241m1\u001b[39m], idx_data[:,\u001b[38;5;241m2\u001b[39m])\n\u001b[1;32m----> 8\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# scheduler.step()\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\colef\\OneDrive - University of Miami\\Documents\\GitHub\\Drug-Repurposing\\.venv\\Lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\colef\\OneDrive - University of Miami\\Documents\\GitHub\\Drug-Repurposing\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\colef\\OneDrive - University of Miami\\Documents\\GitHub\\Drug-Repurposing\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data.to(device)\n",
    "testModel.to(device)\n",
    "testOptimizer = torch.optim.Adam(testModel.parameters(), lr=0.01)\n",
    "testScheduler = StepLR(testOptimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "train(data, ptrain_loader, pval_loader, testModel, testOptimizer, testScheduler, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the embeddings being modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Meaningful embedding '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([ -46.5933,  -66.3595,  -71.1544,  -22.4638,  -56.1134,  -27.2626,\n",
       "         -52.8214, -110.0290], device='cuda:0', grad_fn=<SelectBackward0>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Randomly initialized tensor '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([-0.0067, -0.0112, -0.0135,  0.0087,  0.0057,  0.0015,  0.0065,  0.0107,\n",
       "        -0.0027,  0.0011,  0.0050,  0.0104, -0.0057,  0.0102,  0.0098, -0.0102,\n",
       "        -0.0023, -0.0041, -0.0139,  0.0055,  0.0128, -0.0024, -0.0025,  0.0011,\n",
       "        -0.0114,  0.0112,  0.0014,  0.0124,  0.0072, -0.0099, -0.0014, -0.0123,\n",
       "         0.0093,  0.0028,  0.0014, -0.0006,  0.0099,  0.0052, -0.0079,  0.0010,\n",
       "         0.0067,  0.0063, -0.0114,  0.0101, -0.0113,  0.0089,  0.0022, -0.0046,\n",
       "         0.0005,  0.0136,  0.0118,  0.0077, -0.0100,  0.0067, -0.0021,  0.0041,\n",
       "        -0.0042, -0.0044, -0.0064,  0.0094,  0.0044,  0.0012,  0.0071,  0.0007,\n",
       "        -0.0083,  0.0007,  0.0105, -0.0110,  0.0099,  0.0079,  0.0043, -0.0049,\n",
       "         0.0042,  0.0016,  0.0074,  0.0094, -0.0090, -0.0023, -0.0122, -0.0135,\n",
       "        -0.0092, -0.0136,  0.0013,  0.0036, -0.0107,  0.0105,  0.0144,  0.0091,\n",
       "         0.0045,  0.0071,  0.0107,  0.0094,  0.0041,  0.0048,  0.0095, -0.0036,\n",
       "        -0.0026, -0.0104, -0.0072, -0.0081, -0.0001,  0.0099, -0.0027,  0.0025,\n",
       "         0.0118,  0.0058, -0.0110, -0.0068,  0.0075, -0.0085, -0.0071,  0.0052,\n",
       "        -0.0115,  0.0064,  0.0028,  0.0031,  0.0119,  0.0006, -0.0043, -0.0048,\n",
       "        -0.0113, -0.0024, -0.0144, -0.0078, -0.0092,  0.0103, -0.0091,  0.0134],\n",
       "       device='cuda:0', grad_fn=<SelectBackward0>)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(\"Meaningful embedding \", testModel.Encoder(testModel.data.x_dict,testModel.data.edge_index_dict)['gene_protein'][0])\n",
    "display(\"Randomly initialized tensor \", testModel.data.x_dict['gene_protein'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Edge predictions change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9742, 0.3328, 0.4530], device='cuda:0')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    display(torch.sigmoid(testModel.Decoder(torch.tensor([0,0,2]), torch.tensor([0,1,2]), torch.tensor([2,1,3]))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
